{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430e7f7b",
   "metadata": {},
   "source": [
    "##### ARTI 560 - Computer Vision  \n",
    "## Image Classification using Transfer Learning - Exercise \n",
    "\n",
    "### Objective\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "1. Select another pretrained model (e.g., VGG16, MobileNetV2, or EfficientNet) and fine-tune it for CIFAR-10 classification.  \n",
    "You'll find the pretrained models in [Tensorflow Keras Applications Module](https://www.tensorflow.org/api_docs/python/tf/keras/applications).\n",
    "\n",
    "2. Before training, inspect the architecture using model.summary() and observe:\n",
    "- Network depth\n",
    "- Number of parameters\n",
    "- Trainable vs Frozen layers\n",
    "\n",
    "3. Then compare its performance with ResNet and the custom CNN.\n",
    "\n",
    "### Questions:\n",
    "\n",
    "- Which model achieved the highest accuracy?\n",
    "- Which model trained faster?\n",
    "- How might the architecture explain the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c068d",
   "metadata": {},
   "source": [
    "## All the answers is below the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26d77e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e69771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62cd9c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) Load CIFAR-10\n",
    "# -----------------------------\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "class_names = [\n",
    "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
    "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
    "]\n",
    "\n",
    "# Keep labels as integers (SparseCategoricalCrossentropy)\n",
    "y_train = y_train.squeeze().astype(\"int64\")\n",
    "y_test  = y_test.squeeze().astype(\"int64\")\n",
    "\n",
    "# Convert images to float32\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test  = x_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cc71405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Data augmentation\n",
    "# -----------------------------\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.02),\n",
    "    layers.RandomZoom(0.05),\n",
    "], name=\"augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33503dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Build MobileNetV2 backbone \n",
    "# -----------------------------\n",
    "mobilenet_base = MobileNetV2(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "mobilenet_base.trainable = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e08cbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cifar10_mobilenetv2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"cifar10_mobilenetv2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ augmentation (\u001b[38;5;33mSequential\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resizing_2 (\u001b[38;5;33mResizing\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m12,810\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,270,794</span> (8.66 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,270,794\u001b[0m (8.66 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,810</span> (50.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,810\u001b[0m (50.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4) Full model \n",
    "# -----------------------------\n",
    "mobilenet_model = keras.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    data_augmentation,\n",
    "    layers.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "    layers.Lambda(preprocess_input),          \n",
    "    mobilenet_base,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(10)\n",
    "], name=\"cifar10_mobilenetv2\")\n",
    "\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "434fe4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 222ms/step - accuracy: 0.6042 - loss: 1.1502 - val_accuracy: 0.8158 - val_loss: 0.5250 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 216ms/step - accuracy: 0.7779 - loss: 0.6445 - val_accuracy: 0.8298 - val_loss: 0.4800 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 215ms/step - accuracy: 0.7896 - loss: 0.6042 - val_accuracy: 0.8458 - val_loss: 0.4521 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 215ms/step - accuracy: 0.8021 - loss: 0.5666 - val_accuracy: 0.8380 - val_loss: 0.4643 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 215ms/step - accuracy: 0.8083 - loss: 0.5525 - val_accuracy: 0.8478 - val_loss: 0.4403 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5) Compile + Train (frozen backbone)\n",
    "# -----------------------------\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1),\n",
    "]\n",
    "\n",
    "history = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c6c23a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (frozen) test accuracy: 0.840499997138977\n",
      "MobileNetV2 (frozen) test loss    : 0.46480634808540344\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6) Test / Evaluate\n",
    "# -----------------------------\n",
    "test_loss, test_acc_m = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (frozen) test accuracy:\", test_acc_m)\n",
    "print(\"MobileNetV2 (frozen) test loss    :\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8180003c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total layers in MobileNetV2 backbone: 154\n",
      "Layers with learnable parameters (depth): 104\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of layers inside the MobileNetV2 backbone\n",
    "print(\"Total layers in MobileNetV2 backbone:\", len(mobilenet_base.layers))\n",
    "\n",
    "# Filter only layers that actually have learnable parameters (weights/biases)\n",
    "trainable_layers = [layer for layer in mobilenet_base.layers if layer.count_params() > 0]\n",
    "\n",
    "# Print the number of layers that contain learnable parameters \"Depth of the Model\"\n",
    "print(\"Layers with learnable parameters (depth):\", len(trainable_layers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4c17c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Conv1 864\n",
      "1 bn_Conv1 128\n",
      "2 expanded_conv_depthwise 288\n",
      "3 expanded_conv_depthwise_BN 128\n",
      "4 expanded_conv_project 512\n",
      "5 expanded_conv_project_BN 64\n",
      "6 block_1_expand 1536\n",
      "7 block_1_expand_BN 384\n",
      "8 block_1_depthwise 864\n",
      "9 block_1_depthwise_BN 384\n",
      "10 block_1_project 2304\n",
      "11 block_1_project_BN 96\n",
      "12 block_2_expand 3456\n",
      "13 block_2_expand_BN 576\n",
      "14 block_2_depthwise 1296\n",
      "15 block_2_depthwise_BN 576\n",
      "16 block_2_project 3456\n",
      "17 block_2_project_BN 96\n",
      "18 block_3_expand 3456\n",
      "19 block_3_expand_BN 576\n",
      "20 block_3_depthwise 1296\n",
      "21 block_3_depthwise_BN 576\n",
      "22 block_3_project 4608\n",
      "23 block_3_project_BN 128\n",
      "24 block_4_expand 6144\n",
      "25 block_4_expand_BN 768\n",
      "26 block_4_depthwise 1728\n",
      "27 block_4_depthwise_BN 768\n",
      "28 block_4_project 6144\n",
      "29 block_4_project_BN 128\n",
      "30 block_5_expand 6144\n",
      "31 block_5_expand_BN 768\n",
      "32 block_5_depthwise 1728\n",
      "33 block_5_depthwise_BN 768\n",
      "34 block_5_project 6144\n",
      "35 block_5_project_BN 128\n",
      "36 block_6_expand 6144\n",
      "37 block_6_expand_BN 768\n",
      "38 block_6_depthwise 1728\n",
      "39 block_6_depthwise_BN 768\n",
      "40 block_6_project 12288\n",
      "41 block_6_project_BN 256\n",
      "42 block_7_expand 24576\n",
      "43 block_7_expand_BN 1536\n",
      "44 block_7_depthwise 3456\n",
      "45 block_7_depthwise_BN 1536\n",
      "46 block_7_project 24576\n",
      "47 block_7_project_BN 256\n",
      "48 block_8_expand 24576\n",
      "49 block_8_expand_BN 1536\n",
      "50 block_8_depthwise 3456\n",
      "51 block_8_depthwise_BN 1536\n",
      "52 block_8_project 24576\n",
      "53 block_8_project_BN 256\n",
      "54 block_9_expand 24576\n",
      "55 block_9_expand_BN 1536\n",
      "56 block_9_depthwise 3456\n",
      "57 block_9_depthwise_BN 1536\n",
      "58 block_9_project 24576\n",
      "59 block_9_project_BN 256\n",
      "60 block_10_expand 24576\n",
      "61 block_10_expand_BN 1536\n",
      "62 block_10_depthwise 3456\n",
      "63 block_10_depthwise_BN 1536\n",
      "64 block_10_project 36864\n",
      "65 block_10_project_BN 384\n",
      "66 block_11_expand 55296\n",
      "67 block_11_expand_BN 2304\n",
      "68 block_11_depthwise 5184\n",
      "69 block_11_depthwise_BN 2304\n",
      "70 block_11_project 55296\n",
      "71 block_11_project_BN 384\n",
      "72 block_12_expand 55296\n",
      "73 block_12_expand_BN 2304\n",
      "74 block_12_depthwise 5184\n",
      "75 block_12_depthwise_BN 2304\n",
      "76 block_12_project 55296\n",
      "77 block_12_project_BN 384\n",
      "78 block_13_expand 55296\n",
      "79 block_13_expand_BN 2304\n",
      "80 block_13_depthwise 5184\n",
      "81 block_13_depthwise_BN 2304\n",
      "82 block_13_project 92160\n",
      "83 block_13_project_BN 640\n",
      "84 block_14_expand 153600\n",
      "85 block_14_expand_BN 3840\n",
      "86 block_14_depthwise 8640\n",
      "87 block_14_depthwise_BN 3840\n",
      "88 block_14_project 153600\n",
      "89 block_14_project_BN 640\n",
      "90 block_15_expand 153600\n",
      "91 block_15_expand_BN 3840\n",
      "92 block_15_depthwise 8640\n",
      "93 block_15_depthwise_BN 3840\n",
      "94 block_15_project 153600\n",
      "95 block_15_project_BN 640\n",
      "96 block_16_expand 153600\n",
      "97 block_16_expand_BN 3840\n",
      "98 block_16_depthwise 8640\n",
      "99 block_16_depthwise_BN 3840\n",
      "100 block_16_project 307200\n",
      "101 block_16_project_BN 1280\n",
      "102 Conv_1 409600\n",
      "103 Conv_1_bn 5120\n"
     ]
    }
   ],
   "source": [
    "# Listing all layers that have learnable parameters (trainable_layers)\n",
    "# Each layer will be printed with:\n",
    "# (index in the filtered list, layer name, number of parameters)\n",
    "for i, layer in enumerate(trainable_layers):\n",
    "    print(i, layer.name, layer.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcc2add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers in backbone: 30 / 154\n",
      "Epoch 1/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 280ms/step - accuracy: 0.6961 - loss: 0.8807 - val_accuracy: 0.8400 - val_loss: 0.4837\n",
      "Epoch 2/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 275ms/step - accuracy: 0.8027 - loss: 0.5695 - val_accuracy: 0.8418 - val_loss: 0.4729\n",
      "Epoch 3/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 274ms/step - accuracy: 0.8216 - loss: 0.5118 - val_accuracy: 0.8460 - val_loss: 0.4403\n",
      "Epoch 4/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 275ms/step - accuracy: 0.8334 - loss: 0.4737 - val_accuracy: 0.8576 - val_loss: 0.4062\n",
      "Epoch 5/5\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 275ms/step - accuracy: 0.8529 - loss: 0.4218 - val_accuracy: 0.8680 - val_loss: 0.3765\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Fine-tune last layers\n",
    "# -----------------------------\n",
    "mobilenet_base.trainable = True\n",
    "\n",
    "for layer in mobilenet_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(\"Trainable layers in backbone:\", sum(l.trainable for l in mobilenet_base.layers), \"/\", len(mobilenet_base.layers))\n",
    "\n",
    "mobilenet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = mobilenet_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce6303ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 (fine-tuned) test accuracy: 0.8662999868392944\n",
      "MobileNetV2 (fine-tuned) test loss    : 0.3934086561203003\n"
     ]
    }
   ],
   "source": [
    "test_loss_ft, test_acc_ft = mobilenet_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MobileNetV2 (fine-tuned) test accuracy:\", test_acc_ft)\n",
    "print(\"MobileNetV2 (fine-tuned) test loss    :\", test_loss_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f39ef",
   "metadata": {},
   "source": [
    "# Answers:\n",
    "\n",
    "### -Which model achieved the highest accuracy?\n",
    "\n",
    "ResNet50V2 achieved the highest accuracy (≈ 91.6%), followed by MobileNetV2 (≈ 88.6%), while the custom CNN had the lowest validation performance (≈ 69–70%).\n",
    "\n",
    " ### - Which model trained faster?\n",
    "\n",
    "The custom CNN trained the fastest, followed by MobileNetV2, while ResNet50V2 was the slowest due to its deeper architecture and larger number of parameters.\n",
    "\n",
    "### - How might the architecture explain the differences?\n",
    "\n",
    "*ResNet50V2 is a deep network with residual (skip) connections, allowing it to learn complex features effectively, which leads to higher accuracy.\n",
    "\n",
    "\n",
    "*MobileNetV2 is a lightweight model using depthwise separable convolutions, making it faster but slightly less accurate.\n",
    "\n",
    "\n",
    "*Custom CNN is simpler and trained from scratch, so it lacks pretrained features and tends to overfit, resulting in lower validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d7042d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
